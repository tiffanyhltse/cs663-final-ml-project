{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0ee6d6d2b96d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/original_mfcc_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different possible emotions\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC = df.copy() # make a copy before making edits\n",
    "\n",
    "# label encode the target emotions\n",
    "dfC['label'] = dfC['label'].map({'Angry': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3, 'Neutral': 4, 'Sad': 5})\n",
    "dfC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of target variable\n",
    "dfC['label'].value_counts() # Neutral emotion has least number of records in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate target and features\n",
    "def separate(data):\n",
    "    y = data['label']\n",
    "    X = data.iloc[:, 0:-1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = separate(dfC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_preds = knn.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_preds)\n",
    "print('Accuracy of Baseline KNN: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_preds) # confirm shape of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_preds)\n",
    "plt.figure(figsize=(12,10))\n",
    "cm = pd.DataFrame(cf_matrix, index = [i for i in cate], columns = [i for i in cate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm, linecolor=\"white\", annot=True, linewidth=1, cmap=\"Blues\", fmt='')\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_preds))\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the baseline KNN confusion matrix, we see that the majority of Fear-labeled instances are classified as Happy and Disgust, whereas KNN was able to distinguish quite well between happiness and sadness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model using k-fold cross-validation\n",
    "knn = KNeighborsClassifier()\n",
    "scores = cross_val_score(knn, X, y, cv = 10, scoring = 'accuracy')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10 folds of cross validation, the scores are observed to not significantly vary from the baseline KNN classifier and lie within the range of -/+ 5%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use average accuracy as an estimate of out-of-sample accuracy\n",
    "# numpy array has a method mean()\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for an optimal value of K for KNN\n",
    "\n",
    "# range of k we want to try\n",
    "k_range = range(1, 31)\n",
    "# empty list to store scores\n",
    "k_scores = []\n",
    "\n",
    "# 1. we will loop through reasonable values of k\n",
    "for k in k_range:\n",
    "    # 2. run KNeighborsClassifier with k neighbours\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n",
    "    scores = cross_val_score(knn, X, y, cv = 10, scoring='accuracy')\n",
    "    # 4. append mean of scores for k neighbors to k_scores list\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of list', len(k_scores)) # length of scores should be 30 due to running k-fold cv method 30 times\n",
    "print('Max of list', max(k_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Neighbors (k)')\n",
    "plt.ylabel('Cross-validated accuracy')\n",
    "plt.title('CV Accuracy vs. K Neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the elbow method, we found that 20 neighbors is where the median CV takes place. Let's retrain KNN with the elbow method-verified k value and observe the average cross-validated mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross-validation with best KNN model\n",
    "# calculate mean directly on results\n",
    "knn = KNeighborsClassifier(n_neighbors = 20)\n",
    "y_pred = cross_val_score(knn, X, y, cv = 10, scoring = 'accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as expected, the model with k = 20 neighbors performs slightly better than sklearn's default model with k = 5 neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline SVM classifier\n",
    "best_svc = SVC(kernel = 'rbf')\n",
    "best_svc.fit(X_train, y_train)\n",
    "y_preds = best_svc.predict(X_test)\n",
    "svc_acc = accuracy_score(y_test, y_preds)\n",
    "print('Accuracy of Baseline SVC: ', svc_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_preds)\n",
    "plt.figure(figsize=(12,10))\n",
    "cm = pd.DataFrame(cf_matrix, index = [i for i in cate], columns = [i for i in cate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm, linecolor=\"white\", annot=True, linewidth=1, cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = best_svc.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_preds))\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second algorithm we used is the SVM Classifier, specifically, the non-linear SVM as the boundary the algorithm calculates doesn’t have to be a straight line. We think the algorithm’s kernel trick can help compute a much more optimal hyperplane for our unstructured audio data. The baseline SVC accuracy aligns with our hypothesis that our unstructured data may be better comprehended by selecting the appropriate non-linear kernel function. Our confusion matrix shows Fear having the least quantity of true predictions and are largely misclassified as Happiness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cross validation on SVC\n",
    "best_svc = SVC(kernel = 'rbf')\n",
    "svc_scores = cross_val_score(best_svc, X, y, cv = 10, scoring = 'accuracy')\n",
    "svc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average k-fold score: ', svc_scores.mean())\n",
    "print('Length of list', len(svc_scores)) \n",
    "print('Max of list', max(svc_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum cross-validated score of ~38.8% is above KNN's maximum cross-validated score by around 4%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any improvement using poly kernel?\n",
    "svc2 = SVC(kernel = 'poly')\n",
    "svc_scores2 = cross_val_score(svc2, X, y, cv = 10, scoring = 'accuracy')\n",
    "svc_scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average k-fold score: ', svc_scores2.mean())\n",
    "print('Length of list', len(svc_scores2)) \n",
    "print('Max of list', max(svc_scores2)) # not really any improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A polynomial kernel did not perform significantly worse, but also rarely yields any improvement when compared to the performance of a SVC with RBF kernel. Thus, we decided to stick with the radial basis fuction kernel for re-training with dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the input\n",
    "\n",
    "x_train = StandardScaler().fit_transform(X_train.values)\n",
    "print(x_train)\n",
    "print(len(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(x_train))\n",
    "print(np.std(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on three levels of explained variance (90%, 95%, 98%)\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of components equal to number of input features\n",
    "pca = PCA().fit(x_train) # fit as many principal components possible\n",
    "\n",
    "# confirm findings by how much variance is explained by the principal components derived\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "explained_variance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse of scree plot\n",
    "plt.plot(np.cumsum(explained_variance_ratio))\n",
    "plt.title('Cumulative Explained Variance per Principal Component')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(explained_variance_ratio[:65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refit(train_data, test_data, n_components):\n",
    "    refit_pca = PCA(n_components = n_components)\n",
    "    PCA_X_train = refit_pca.fit_transform(train_data)\n",
    "    PCA_X_test = refit_pca.transform(test_data)\n",
    "    \n",
    "    return PCA_X_train, PCA_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = StandardScaler().fit_transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_train1, pca_X_test1 = refit(x_train, x_test, 17)\n",
    "pca_X_train2, pca_X_test2 = refit(x_train, x_test, 34)\n",
    "pca_X_train3, pca_X_test3 = refit(x_train, x_test, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca_X_test3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC keeping only 17 components\n",
    "final_svm_model = SVC(kernel = 'rbf').fit(pca_X_train1, y_train)\n",
    "final_pred_train = final_svm_model.predict(pca_X_train1)\n",
    "final_pred_test = final_svm_model.predict(pca_X_test1)\n",
    "print('Test Data Accuracy after PCA Transformation: ', accuracy_score(y_test, final_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC keeping first 34 components\n",
    "final_svm_model2 = SVC(kernel = 'rbf').fit(pca_X_train2, y_train)\n",
    "final_pred_train2 = final_svm_model2.predict(pca_X_train2)\n",
    "final_pred_test2 = final_svm_model2.predict(pca_X_test2)\n",
    "print('Test Data Accuracy after PCA Transformation: ', accuracy_score(y_test, final_pred_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC keeping first 65 components\n",
    "final_svm_model3 = SVC(kernel = 'rbf').fit(pca_X_train3, y_train)\n",
    "final_pred_train3 = final_svm_model3.predict(pca_X_train3)\n",
    "final_pred_test3 = final_svm_model3.predict(pca_X_test3)\n",
    "print('Test Data Accuracy after PCA Transformation: ', accuracy_score(y_test, final_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the slight variation in performance each time the SVM model is refit with the corresponding number of PCA components, we observed that most of the time, it is ideal to keep only the first 17 or 34 components given its boost in performance from our very baseline accuracy of 34.5% to 37.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated SVM w/PCA\n",
    "svc_w_pca = SVC(kernel = 'rbf')\n",
    "svc_pca_scores = cross_val_score(svc_w_pca, X, y, cv = 10, scoring = 'accuracy')\n",
    "print(svc_pca_scores)\n",
    "print('Average k-fold score with RBF kernel: ', svc_pca_scores.mean())\n",
    "print('Length of list', len(svc_pca_scores)) \n",
    "print('Max of list', max(svc_pca_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validated SVM w/PCA\n",
    "svc_w_pca2 = SVC(kernel = 'poly')\n",
    "svc_pca_scores2 = cross_val_score(svc_w_pca2, X, y, cv = 10, scoring = 'accuracy')\n",
    "print(svc_pca_scores2)\n",
    "print('Average k-fold score with poly kernel: ', svc_pca_scores2.mean())\n",
    "print('Length of list', len(svc_pca_scores2)) \n",
    "print('Max of list', max(svc_pca_scores2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When cross validation is performed with SVM + PCA combined, it can be seen that the new combined method with the RBF kernel still yields slightly better performance than the same with a polynomial kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3D(input_data):\n",
    "    print(\"Before reshaping: \", input_data.shape)\n",
    "    X = []\n",
    "    for row in input_data: \n",
    "        X.append([row])\n",
    "    X = np.asarray(X)\n",
    "    print(\"After reshaping: \", X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.iloc[:, :-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_3D = to_3D(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df.iloc[:, -1].to_numpy()\n",
    "le = LabelEncoder()\n",
    "targets_arr = le.fit_transform(targets)\n",
    "targets_arr = to_categorical(targets_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_3D, targets_arr, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(1, 216)))\n",
    "    model.add(SpatialDropout1D(0.1))\n",
    "    model.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    return modelmodel = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(patience=5,restore_best_weights=True)\n",
    "\n",
    "checkpoint_path = \"ckpts/cp_fs.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "         validation_split=0.15,\n",
    "         callbacks=[cp_callback, earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss: {:0.3f}\\nAccuracy: {:0.3f}'.format(accuracy[0],accuracy[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict_classes(X_test)\n",
    "label = np.where(y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(labels=label, predictions=prediction, num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(confusion.numpy(), index = [i for i in cate] , columns = [i for i in cate])\n",
    "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
